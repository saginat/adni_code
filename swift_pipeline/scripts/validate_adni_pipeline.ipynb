{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d48d0354",
   "metadata": {},
   "source": [
    "# SwiFT Pipeline with ADNI Degradation Data\n",
    "\n",
    "This notebook demonstrates the complete SwiFT (Swin 4D fMRI Transformer) pipeline applied to ADNI (Alzheimer's Disease Neuroimaging Initiative) data for cognitive decline prediction.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The notebook covers:\n",
    "1. **Setup and Configuration** - Import libraries and configure paths\n",
    "2. **Stage 1: Contrastive Pretraining** - Self-supervised learning on unlabeled fMRI data\n",
    "3. **Stage 2: Supervised Fine-tuning** - Training on degradation prediction tasks\n",
    "4. **Evaluation** - Test set evaluation and metrics\n",
    "\n",
    "## SwiFT Methodology\n",
    "\n",
    "SwiFT uses a two-stage approach:\n",
    "- **Pretraining**: Learn robust fMRI representations using contrastive learning (no labels needed)\n",
    "- **Fine-tuning**: Adapt the pretrained model to specific downstream tasks (degradation prediction)\n",
    "\n",
    "This approach allows efficient use of limited labeled data by leveraging unlabeled data during pretraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e072bf",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd6132",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8bfc37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(x, '/sci/nosnap/arieljaffe/sagi.nathan/shared_fmri_data/all_4d_downsampled_35x35x35.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60dce67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SWIFT PIPELINE WITH ADNI DEGRADATION DATA\n",
      "================================================================================\n",
      "\n",
      "✓ All imports successful!\n",
      "PyTorch version: 2.8.0+cu128\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA L4\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "\n",
    "# SwiFT pipeline imports\n",
    "from data.prepare_adni_data import prepare_adni_datasets, create_adni_dataloaders\n",
    "from models.swin4d_transformer_ver7 import SwinTransformer4D\n",
    "from models.heads import ContrastiveHead, ClassificationHead\n",
    "from training.losses import NTXentLoss\n",
    "from configs.config_pretrain import MODEL_CONFIG, CONTRASTIVE_CONFIG, TRAIN_CONFIG, DATA_CONFIG\n",
    "from configs.config_finetune import TASK_CONFIG, HEAD_CONFIG, TRAIN_CONFIG as FINETUNE_TRAIN_CONFIG\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SWIFT PIPELINE WITH ADNI DEGRADATION DATA\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n✓ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0043c4",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set up paths to your ADNI data files and training parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf85470",
   "metadata": {},
   "source": [
    "# Paths to your ADNI data\n",
    "# Updated to use shared_fmri_data with new resolution: 35x37x35x10\n",
    "USE_DUMMY_DATA = False  # Using real data from shared_fmri_data\n",
    "\n",
    "if USE_DUMMY_DATA:\n",
    "    BASE_DATA_PATH = \"../data_dummy\"\n",
    "    DATA_PATH = f\"{BASE_DATA_PATH}/all_4d_downsampled.pt\"\n",
    "    LABELS_PATH = f\"{BASE_DATA_PATH}/imageID_to_labels.json\"\n",
    "    INFO_PATH = f\"{BASE_DATA_PATH}/index_to_name.json\"\n",
    "    print(\"⚠️  Using DUMMY data for testing\")\n",
    "    print(\"   Run 'create_dummy_adni_data.py' first to generate dummy data\")\n",
    "else:\n",
    "    # Using real ADNI data from shared_fmri_data\n",
    "    BASE_DATA_PATH = \"/sci/nosnap/arieljaffe/sagi.nathan\"\n",
    "    DATA_PATH = f\"{BASE_DATA_PATH}/shared_fmri_data/all_4d_downsampled.pt\"\n",
    "    LABELS_PATH = f\"{BASE_DATA_PATH}/imageID_to_labels.json\"\n",
    "    INFO_PATH = f\"{BASE_DATA_PATH}/index_to_name.json\"\n",
    "    print(\"✓ Using REAL ADNI data from shared_fmri_data\")\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "\n",
    "# Task configuration - choose which degradation task to predict\n",
    "DEGRADATION_TASKS = [\n",
    "    \"degradation_binary_1year\",\n",
    "    # \"degradation_binary_2years\",  # Uncomment for multi-task learning\n",
    "    # \"degradation_binary_3years\",\n",
    "]\n",
    "\n",
    "# Spatial configuration - Updated for new resolution\n",
    "# Using Resize3D with scale_factor=0.7 to achieve ~35x37x35 spatial resolution\n",
    "TARGET_SPATIAL_SIZE = (35, 37, 35)  # Model expects 35x37x35 input (changed from 96x96x96)\n",
    "SCALE_FACTOR = 0.7  # Resize3D scale factor to achieve target spatial size\n",
    "WINDOW_SIZE = 10  # Temporal window size (changed from 20)\n",
    "STRIDE = 5  # Temporal stride (changed from 10)\n",
    "\n",
    "# Training configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PRETRAIN_EPOCHS = 5  # Increase for real training (e.g., 100-300)\n",
    "FINETUNE_EPOCHS = 10  # Increase for real training (e.g., 50-100)\n",
    "\n",
    "print(f\"\\nDevice: {DEVICE}\")\n",
    "print(f\"Target spatial size: {TARGET_SPATIAL_SIZE}\")\n",
    "print(f\"Scale factor: {SCALE_FACTOR}\")\n",
    "print(f\"Temporal window size: {WINDOW_SIZE}\")\n",
    "print(f\"Temporal stride: {STRIDE}\")\n",
    "print(f\"Task(s): {DEGRADATION_TASKS}\")\n",
    "print(f\"Pretraining epochs: {PRETRAIN_EPOCHS}\")\n",
    "print(f\"Fine-tuning epochs: {FINETUNE_EPOCHS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7612b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using REAL preprocessed ADNI data\n",
      "Preprocessed directory: /sci/nosnap/arieljaffe/sagi.nathan/shared_fmri_data/preprocessed_windows\n",
      "\n",
      "Device: cuda\n",
      "Task(s): ['degradation_binary_1year']\n",
      "Pretraining epochs: 5\n",
      "Fine-tuning epochs: 10\n"
     ]
    }
   ],
   "source": [
    "# Paths to your ADNI data\n",
    "# Updated to use disk-based preprocessed windows\n",
    "USE_DUMMY_DATA = False  # Using real preprocessed data\n",
    "\n",
    "if USE_DUMMY_DATA:\n",
    "    BASE_DATA_PATH = \"../data_dummy\"\n",
    "    PREPROCESSED_DIR = f\"{BASE_DATA_PATH}/preprocessed_windows\"\n",
    "    LABELS_PATH = f\"{BASE_DATA_PATH}/imageID_to_labels.json\"\n",
    "    INFO_PATH = f\"{BASE_DATA_PATH}/index_to_name.json\"\n",
    "    print(\"⚠️  Using DUMMY data for testing\")\n",
    "    print(\"   Run 'create_dummy_adni_data.py' first to generate dummy data\")\n",
    "else:\n",
    "    # Using real preprocessed ADNI data\n",
    "    BASE_DATA_PATH = \"/sci/nosnap/arieljaffe/sagi.nathan/shared_fmri_data\"\n",
    "    PREPROCESSED_DIR = f\"{BASE_DATA_PATH}/preprocessed_windows\"\n",
    "    LABELS_PATH = f\"{PREPROCESSED_DIR}/imageID_to_labels.json\"\n",
    "    INFO_PATH = f\"{PREPROCESSED_DIR}/index_to_info.json\"\n",
    "    print(\"✓ Using REAL preprocessed ADNI data\")\n",
    "\n",
    "print(f\"Preprocessed directory: {PREPROCESSED_DIR}\")\n",
    "\n",
    "# Task configuration - choose which degradation task to predict\n",
    "DEGRADATION_TASKS = [\n",
    "    \"degradation_binary_1year\",\n",
    "    # \"degradation_binary_2years\",  # Uncomment for multi-task learning\n",
    "    # \"degradation_binary_3years\",\n",
    "]\n",
    "\n",
    "# Training configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PRETRAIN_EPOCHS = 5  # Increase for real training (e.g., 100-300)\n",
    "FINETUNE_EPOCHS = 10  # Increase for real training (e.g., 50-100)\n",
    "\n",
    "print(f\"\\nDevice: {DEVICE}\")\n",
    "print(f\"Task(s): {DEGRADATION_TASKS}\")\n",
    "print(f\"Pretraining epochs: {PRETRAIN_EPOCHS}\")\n",
    "print(f\"Fine-tuning epochs: {FINETUNE_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7a45fb",
   "metadata": {},
   "source": [
    "## 3. Stage 1: Contrastive Pretraining (Self-Supervised)\n",
    "\n",
    "In this stage, we train the SwiFT encoder using contrastive learning **without labels**. The model learns to create meaningful representations of fMRI data by:\n",
    "- Taking two different temporal windows from the same scan\n",
    "- Learning to make their representations similar (positive pairs)\n",
    "- While making representations from different scans dissimilar (negative pairs)\n",
    "\n",
    "This is self-supervised learning - no degradation labels are needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c8d6e6",
   "metadata": {},
   "source": [
    "### 3.1 Prepare Pretraining Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1162603f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36fbbbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 1: CONTRASTIVE PRETRAINING\n",
      "================================================================================\n",
      "================================================================================\n",
      "PREPARING DISK-BASED ADNI DATASETS FOR SWIFT PIPELINE\n",
      "================================================================================\n",
      "\n",
      "Loading window index from /sci/nosnap/arieljaffe/sagi.nathan/shared_fmri_data/preprocessed_windows/window_index.json...\n",
      "  Total windows: 18792\n",
      "\n",
      "Loading labels from /sci/nosnap/arieljaffe/sagi.nathan/shared_fmri_data/preprocessed_windows/imageID_to_labels.json...\n",
      "  Labels for 829 scans\n",
      "\n",
      "Data split by subjects (no data leakage):\n",
      "  Train: 488 subjects, 13176 samples\n",
      "  Val:   69 subjects, 1863 samples\n",
      "  Test:  139 subjects, 3753 samples\n",
      "\n",
      "================================================================================\n",
      "CREATING CONTRASTIVE PRETRAINING DATASETS\n",
      "================================================================================\n",
      "  Contrastive dataset: 13176 valid samples from 488 scans\n",
      "  Contrastive dataset: 1863 valid samples from 69 scans\n",
      "\n",
      "================================================================================\n",
      "DATA PREPARATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "\n",
      "✓ Dataloaders created:\n",
      "  - Train batches: 3294\n",
      "  - Val batches: 466\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 1: CONTRASTIVE PRETRAINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare datasets for pretraining using disk-based loading\n",
    "from data.dataset_adni_disk import prepare_adni_disk_datasets\n",
    "\n",
    "pretrain_datasets = prepare_adni_disk_datasets(\n",
    "    preprocessed_dir=PREPROCESSED_DIR,\n",
    "    task_names=DEGRADATION_TASKS,  # Not used in pretraining\n",
    "    stage=\"pretrain\",\n",
    "    val_split=0.1,\n",
    "    test_split=0.2,\n",
    "    seed=42,\n",
    "    target_spatial_size=None,  # Already preprocessed\n",
    "    handle_nan='skip',\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    pretrain_datasets['train_dataset'],\n",
    "    batch_size=TRAIN_CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=4,  # Parallel loading\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    pretrain_datasets['val_dataset'],\n",
    "    batch_size=TRAIN_CONFIG[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Dataloaders created:\")\n",
    "print(f\"  - Train batches: {len(train_loader)}\")\n",
    "print(f\"  - Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08c432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff221f56",
   "metadata": {},
   "source": [
    "### 3.2 Initialize Models for Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6fec02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models for pretraining...\n",
      "img_size:  (35, 37, 35, 10)\n",
      "patch_size:  (5, 5, 5, 1)\n",
      "patch_dim:  (7, 7, 7, 10)\n",
      "\n",
      "✓ Models initialized on cuda\n",
      "  - Encoder parameters: 4,125,132\n",
      "  - Contrastive head parameters: 120,800\n",
      "✓ Loss function: NTXentLoss (temperature=0.5)\n",
      "✓ Optimizer: AdamW (lr=5e-05)\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing models for pretraining...\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SwiFT Configuration (adapted for ADNI data)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  img_size:           {MODEL_CONFIG['img_size']}\")\n",
    "print(f\"  patch_size:         {MODEL_CONFIG['patch_size']}\")\n",
    "print(f\"  window_size:        {MODEL_CONFIG['window_size']}\")\n",
    "print(f\"  first_window_size:  {MODEL_CONFIG['first_window_size']}\")\n",
    "print(f\"  embed_dim:          {MODEL_CONFIG['embed_dim']}\")\n",
    "print(f\"  depths:             {MODEL_CONFIG['depths']}\")\n",
    "print(f\"  num_heads:          {MODEL_CONFIG['num_heads']}\")\n",
    "print(f\"  c_multiplier:       {MODEL_CONFIG['c_multiplier']}\")\n",
    "print(f\"  last_layer_full_MSA:{MODEL_CONFIG['last_layer_full_MSA']}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  batch_size:         {TRAIN_CONFIG['batch_size']}\")\n",
    "print(f\"  learning_rate:      {TRAIN_CONFIG['learning_rate']}\")\n",
    "print(f\"  temperature:        {TRAIN_CONFIG['temperature']}\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "# SwiFT Encoder (4D Swin Transformer)\n",
    "encoder = SwinTransformer4D(**MODEL_CONFIG).to(DEVICE)\n",
    "\n",
    "# Contrastive projection head\n",
    "contrastive_head = ContrastiveHead(**CONTRASTIVE_CONFIG).to(DEVICE)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in encoder.parameters())\n",
    "head_params = sum(p.numel() for p in contrastive_head.parameters())\n",
    "print(f\"\\n✓ Models initialized on {DEVICE}\")\n",
    "print(f\"  - Encoder parameters: {total_params:,}\")\n",
    "print(f\"  - Contrastive head parameters: {head_params:,}\")\n",
    "\n",
    "# Setup training with SwiFT defaults\n",
    "criterion = NTXentLoss(\n",
    "    device=DEVICE,\n",
    "    batch_size=TRAIN_CONFIG[\"batch_size\"],  # SwiFT default: 8\n",
    "    temperature=TRAIN_CONFIG[\"temperature\"],  # 0.1\n",
    ")\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    list(encoder.parameters()) + list(contrastive_head.parameters()),\n",
    "    lr=TRAIN_CONFIG[\"learning_rate\"],  # SwiFT default: 5e-5\n",
    ")\n",
    "\n",
    "print(f\"✓ Loss function: NTXentLoss (temperature={TRAIN_CONFIG['temperature']})\")\n",
    "print(f\"✓ Optimizer: AdamW (lr={TRAIN_CONFIG['learning_rate']}, batch_size={TRAIN_CONFIG['batch_size']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319978fe",
   "metadata": {},
   "source": [
    "### 3.3 Run Contrastive Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09129279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting pretraining (5 epochs)...\n",
      "Note: Increase epochs for real training (e.g., 100-300)\n",
      "\n",
      "  Epoch 1/5: Loss = 1.9540\n",
      "  Epoch 1/5: Loss = 1.9540\n",
      "  Epoch 2/5: Loss = 1.9362\n",
      "  Epoch 2/5: Loss = 1.9362\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Encode both views\u001b[39;00m\n\u001b[0;32m     18\u001b[0m features1 \u001b[38;5;241m=\u001b[39m encoder(view1)\n\u001b[1;32m---> 19\u001b[0m features2 \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mview2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Project to embedding space\u001b[39;00m\n\u001b[0;32m     22\u001b[0m embeddings1 \u001b[38;5;241m=\u001b[39m contrastive_head(features1)\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\Y\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\my_code\\swift_pipeline\\models\\swin4d_transformer_ver7.py:894\u001b[0m, in \u001b[0;36mSwinTransformer4D.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m    893\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embeds[i](x)\n\u001b[1;32m--> 894\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;66;03m# moved this part to clf_mlp or reg_mlp\u001b[39;00m\n\u001b[0;32m    897\u001b[0m \n\u001b[0;32m    898\u001b[0m \u001b[38;5;66;03m# x = x.flatten(start_dim=2).transpose(1, 2)  # B L C\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# x = torch.flatten(x, 1)\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;66;03m# x = self.head(x)\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\Y\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\my_code\\swift_pipeline\\models\\swin4d_transformer_ver7.py:526\u001b[0m, in \u001b[0;36mBasicLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    524\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m compute_mask([dp, hp, wp, tp], window_size, shift_size, x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m--> 526\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(b, d, h, w, t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\Y\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\my_code\\swift_pipeline\\models\\swin4d_transformer_ver7.py:330\u001b[0m, in \u001b[0;36mSwinTransformerBlock4D.forward\u001b[1;34m(self, x, mask_matrix)\u001b[0m\n\u001b[0;32m    328\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_part1, x, mask_matrix)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 330\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_part1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m x \u001b[38;5;241m=\u001b[39m shortcut \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(x)\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_checkpoint:\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\my_code\\swift_pipeline\\models\\swin4d_transformer_ver7.py:304\u001b[0m, in \u001b[0;36mSwinTransformerBlock4D.forward_part1\u001b[1;34m(self, x, mask_matrix)\u001b[0m\n\u001b[0;32m    302\u001b[0m     attn_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    303\u001b[0m x_windows \u001b[38;5;241m=\u001b[39m window_partition(shifted_x, window_size)\n\u001b[1;32m--> 304\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m attn_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m(window_size \u001b[38;5;241m+\u001b[39m (c,)))\n\u001b[0;32m    306\u001b[0m shifted_x \u001b[38;5;241m=\u001b[39m window_reverse(attn_windows, window_size, dims)\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\Y\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\my_code\\swift_pipeline\\models\\swin4d_transformer_ver7.py:199\u001b[0m, in \u001b[0;36mWindowAttention4D.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    195\u001b[0m     attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mview(b_ \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m nw, nw, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, n, n) \u001b[38;5;241m+\u001b[39m mask\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m    196\u001b[0m         attn\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    197\u001b[0m     )\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    198\u001b[0m     attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, n, n)\n\u001b[1;32m--> 199\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(attn)\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\Y\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\Y\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1482\u001b[0m, in \u001b[0;36mSoftmax.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\Y\\envs\\py39\\lib\\site-packages\\torch\\nn\\functional.py:1843\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1843\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1844\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1845\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Create timestamped checkpoint directory\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_dir = f\"../checkpoints/pretrain_run_{timestamp}\"\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "print(f\"\\nCreated run directory: {run_dir}\")\n",
    "print(f\"Starting pretraining ({PRETRAIN_EPOCHS} epochs)...\")\n",
    "print(\"Note: Increase epochs for real training (e.g., 100-300)\\n\")\n",
    "\n",
    "encoder.train()\n",
    "contrastive_head.train()\n",
    "\n",
    "# Calculate checkpoint intervals (at 25%, 50%, 75%, 100%)\n",
    "checkpoint_intervals = [0.25, 0.50, 0.75, 1.0]\n",
    "total_steps = PRETRAIN_EPOCHS\n",
    "checkpoint_steps = [int(total_steps * interval) for interval in checkpoint_intervals]\n",
    "\n",
    "for epoch in range(PRETRAIN_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, (view1, view2) in enumerate(train_loader):\n",
    "        view1, view2 = view1.to(DEVICE), view2.to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Encode both views\n",
    "        features1 = encoder(view1)\n",
    "        features2 = encoder(view2)\n",
    "        \n",
    "        # Project to embedding space\n",
    "        embeddings1 = contrastive_head(features1)\n",
    "        embeddings2 = contrastive_head(features2)\n",
    "        \n",
    "        # Compute contrastive loss\n",
    "        loss = criterion(embeddings1, embeddings2)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Limit batches for demo (remove for real training)\n",
    "        if num_batches >= 10:\n",
    "            break\n",
    "    \n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    print(f\"  Epoch {epoch+1}/{PRETRAIN_EPOCHS}: Loss = {avg_loss:.4f}\")\n",
    "    \n",
    "    # Save checkpoints at 25%, 50%, 75%, and 100%\n",
    "    current_epoch = epoch + 1\n",
    "    if current_epoch in checkpoint_steps:\n",
    "        checkpoint_idx = checkpoint_steps.index(current_epoch)\n",
    "        checkpoint_name = f\"checkpoint_{(checkpoint_idx + 1) * 25}percent_epoch{current_epoch}.pth\"\n",
    "        checkpoint_path = os.path.join(run_dir, checkpoint_name)\n",
    "        \n",
    "        torch.save({\n",
    "            'model_state_dict': encoder.state_dict(),\n",
    "            'head_state_dict': contrastive_head.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'epoch': current_epoch,\n",
    "            'loss': avg_loss,\n",
    "            'config': MODEL_CONFIG,\n",
    "        }, checkpoint_path)\n",
    "        \n",
    "        print(f\"    ✓ Saved checkpoint: {checkpoint_name}\")\n",
    "\n",
    "print(\"\\n✓ Pretraining complete!\")\n",
    "print(\"  The encoder has learned to create meaningful fMRI representations\")\n",
    "print(f\"  All checkpoints saved to: {run_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca44c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9cf3482",
   "metadata": {},
   "source": [
    "### 3.4 Save Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5a93bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved pretrained encoder to: ../checkpoints/pretrained_adni.pth\n",
      "  Note: Only the encoder is saved, not the contrastive head\n"
     ]
    }
   ],
   "source": [
    "pretrained_path = os.path.join(run_dir, \"pretrained_adni_final.pth\")\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': encoder.state_dict(),\n",
    "    'head_state_dict': contrastive_head.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'config': MODEL_CONFIG,\n",
    "}, pretrained_path)\n",
    "\n",
    "print(f\"✓ Saved pretrained encoder to: {pretrained_path}\")\n",
    "print(f\"  Run directory: {run_dir}\")\n",
    "print(\"  Note: Only the encoder is saved, not the contrastive head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4928f2d0",
   "metadata": {},
   "source": [
    "## 4. Stage 2: Supervised Fine-tuning for Degradation Prediction\n",
    "\n",
    "Now we use the pretrained encoder for the downstream task: predicting cognitive decline (degradation).\n",
    "\n",
    "Key differences from pretraining:\n",
    "- **Uses labels**: We now have degradation labels (0 or 1) from `imageID_to_labels.json`\n",
    "- **Frozen encoder**: The pretrained encoder is typically frozen (not trained)\n",
    "- **Classification head**: We train only a small classification head on top\n",
    "- **Fewer epochs needed**: Transfer learning converges faster than training from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef7a33",
   "metadata": {},
   "source": [
    "### 4.1 Prepare Fine-tuning Datasets with Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 2: SUPERVISED FINE-TUNING\n",
      "================================================================================\n",
      "================================================================================\n",
      "PREPARING ADNI DATA FOR SWIFT PIPELINE\n",
      "================================================================================\n",
      "Loading ADNI data...\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 2: SUPERVISED FINE-TUNING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare datasets for finetuning with labels\n",
    "finetune_datasets = prepare_adni_datasets(\n",
    "    data_path=DATA_PATH,\n",
    "    labels_path=LABELS_PATH,\n",
    "    info_path=INFO_PATH,\n",
    "    task_names=DEGRADATION_TASKS,\n",
    "    stage=\"finetune\",\n",
    "    val_split=0.1,\n",
    "    test_split=0.2,\n",
    "    seed=42,\n",
    "    target_spatial_size=TARGET_SPATIAL_SIZE,\n",
    "    scale_factor=SCALE_FACTOR,\n",
    "    window_size=WINDOW_SIZE,\n",
    "    stride=STRIDE,\n",
    "    handle_nan='skip',  # Skip samples with NaN labels\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "finetune_loaders = create_adni_dataloaders(\n",
    "    finetune_datasets,\n",
    "    batch_size=FINETUNE_TRAIN_CONFIG[\"batch_size\"],\n",
    "    shuffle_train=True,\n",
    ")\n",
    "\n",
    "train_loader_ft = finetune_loaders[\"train_loader\"]\n",
    "val_loader_ft = finetune_loaders[\"val_loader\"]\n",
    "test_loader_ft = finetune_loaders[\"test_loader\"]\n",
    "\n",
    "print(f\"\\n✓ Dataloaders created:\")\n",
    "print(f\"  - Train batches: {len(train_loader_ft)}\")\n",
    "print(f\"  - Val batches: {len(val_loader_ft)}\")\n",
    "print(f\"  - Test batches: {len(test_loader_ft)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a04ec8f",
   "metadata": {},
   "source": [
    "### 4.2 Load Pretrained Encoder and Create Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02bdbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading pretrained encoder...\")\n",
    "finetuned_encoder = SwinTransformer4D(**MODEL_CONFIG).to(DEVICE)\n",
    "checkpoint = torch.load(pretrained_path, map_location=DEVICE)\n",
    "finetuned_encoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\"✓ Loaded pretrained weights\")\n",
    "\n",
    "# Freeze encoder if specified\n",
    "if TASK_CONFIG['freeze_encoder']:\n",
    "    print(\"\\nFreezing encoder weights...\")\n",
    "    for param in finetuned_encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    finetuned_encoder.eval()\n",
    "    \n",
    "    trainable = sum(p.numel() for p in finetuned_encoder.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in finetuned_encoder.parameters())\n",
    "    print(f\"  - Trainable encoder parameters: {trainable:,} / {total:,}\")\n",
    "else:\n",
    "    print(\"\\nEncoder weights will be fine-tuned (not frozen)\")\n",
    "\n",
    "# Create classification head\n",
    "num_tasks = len(DEGRADATION_TASKS)\n",
    "classification_head = ClassificationHead(\n",
    "    num_classes=2,  # Binary classification\n",
    "    num_features=HEAD_CONFIG['num_features']\n",
    ").to(DEVICE)\n",
    "\n",
    "head_params = sum(p.numel() for p in classification_head.parameters())\n",
    "print(f\"\\n✓ Classification head created\")\n",
    "print(f\"  - Head parameters: {head_params:,}\")\n",
    "print(f\"  - Task(s): {DEGRADATION_TASKS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f0f5b3",
   "metadata": {},
   "source": [
    "### 4.3 Setup Training for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68703ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer (only for classification head if encoder is frozen)\n",
    "if TASK_CONFIG['freeze_encoder']:\n",
    "    params_to_optimize = classification_head.parameters()\n",
    "    print(\"Optimizing: Classification head only (encoder frozen)\")\n",
    "else:\n",
    "    params_to_optimize = list(finetuned_encoder.parameters()) + list(classification_head.parameters())\n",
    "    print(\"Optimizing: Both encoder and classification head\")\n",
    "\n",
    "optimizer_ft = optim.AdamW(\n",
    "    params_to_optimize,\n",
    "    lr=FINETUNE_TRAIN_CONFIG[\"learning_rate\"],\n",
    "    weight_decay=FINETUNE_TRAIN_CONFIG[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "# Loss function for binary classification\n",
    "criterion_ft = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(f\"\\n✓ Setup complete:\")\n",
    "print(f\"  - Optimizer: AdamW\")\n",
    "print(f\"  - Learning rate: {FINETUNE_TRAIN_CONFIG['learning_rate']}\")\n",
    "print(f\"  - Loss function: BCEWithLogitsLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11783cdd",
   "metadata": {},
   "source": [
    "### 4.4 Run Fine-tuning Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6b614",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nStarting fine-tuning ({FINETUNE_EPOCHS} epochs)...\")\n",
    "classification_head.train()\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(FINETUNE_EPOCHS):\n",
    "    # Training phase\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader_ft):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE).float().squeeze()\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer_ft.zero_grad()\n",
    "        \n",
    "        with torch.set_grad_enabled(not TASK_CONFIG['freeze_encoder']):\n",
    "            features = finetuned_encoder(inputs)\n",
    "        \n",
    "        outputs = classification_head(features).squeeze()\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion_ft(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer_ft.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        train_loss += loss.item()\n",
    "        predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "    \n",
    "    # Validation phase\n",
    "    classification_head.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader_ft:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE).float().squeeze()\n",
    "            \n",
    "            features = finetuned_encoder(inputs)\n",
    "            outputs = classification_head(features).squeeze()\n",
    "            \n",
    "            loss = criterion_ft(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            val_correct += (predictions == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "    \n",
    "    # Calculate epoch metrics\n",
    "    train_loss /= len(train_loader_ft)\n",
    "    train_acc = 100 * train_correct / train_total\n",
    "    val_loss /= len(val_loader_ft)\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    \n",
    "    print(f\"  Epoch {epoch+1}/{FINETUNE_EPOCHS}: \"\n",
    "          f\"Train Loss={train_loss:.4f}, Acc={train_acc:.2f}% | \"\n",
    "          f\"Val Loss={val_loss:.4f}, Acc={val_acc:.2f}%\")\n",
    "    \n",
    "    # Track best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch + 1\n",
    "    \n",
    "    classification_head.train()\n",
    "\n",
    "print(f\"\\n✓ Fine-tuning complete!\")\n",
    "print(f\"  Best validation accuracy: {best_val_acc:.2f}% at epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fe26f2",
   "metadata": {},
   "source": [
    "## 5. Evaluation on Test Set\n",
    "\n",
    "Evaluate the fine-tuned model on the held-out test set to measure final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION ON TEST SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "classification_head.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader_ft:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE).float().squeeze()\n",
    "        \n",
    "        features = finetuned_encoder(inputs)\n",
    "        outputs = classification_head(features).squeeze()\n",
    "        \n",
    "        loss = criterion_ft(outputs, labels)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "        predictions = (probabilities > 0.5).float()\n",
    "        test_correct += (predictions == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader_ft)\n",
    "test_acc = 100 * test_correct / test_total\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  - Loss: {test_loss:.4f}\")\n",
    "print(f\"  - Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"  - Correct predictions: {test_correct}/{test_total}\")\n",
    "print(f\"  - Task: {DEGRADATION_TASKS[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d605f4",
   "metadata": {},
   "source": [
    "### 5.1 Compute Additional Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e415793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert to numpy arrays\n",
    "predictions_np = np.array(all_predictions)\n",
    "labels_np = np.array(all_labels)\n",
    "\n",
    "# Calculate confusion matrix metrics\n",
    "tp = ((predictions_np == 1) & (labels_np == 1)).sum()\n",
    "tn = ((predictions_np == 0) & (labels_np == 0)).sum()\n",
    "fp = ((predictions_np == 1) & (labels_np == 0)).sum()\n",
    "fn = ((predictions_np == 0) & (labels_np == 1)).sum()\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "print(f\"  - Precision: {precision:.4f}\")\n",
    "print(f\"  - Recall: {recall:.4f}\")\n",
    "print(f\"  - F1-Score: {f1:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  - True Positives: {tp}\")\n",
    "print(f\"  - True Negatives: {tn}\")\n",
    "print(f\"  - False Positives: {fp}\")\n",
    "print(f\"  - False Negatives: {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccffabd",
   "metadata": {},
   "source": [
    "## 6. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2e1d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_path = \"../checkpoints/finetuned_degradation.pth\"\n",
    "\n",
    "torch.save({\n",
    "    'encoder_state_dict': finetuned_encoder.state_dict(),\n",
    "    'head_state_dict': classification_head.state_dict(),\n",
    "    'config': {\n",
    "        'model_config': MODEL_CONFIG,\n",
    "        'task_config': TASK_CONFIG,\n",
    "        'tasks': DEGRADATION_TASKS,\n",
    "    },\n",
    "    'results': {\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "}, final_path)\n",
    "\n",
    "print(f\"✓ Saved final model to: {final_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340baf72",
   "metadata": {},
   "source": [
    "## 7. Pipeline Summary\n",
    "\n",
    "Complete overview of the SwiFT pipeline results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SWIFT PIPELINE COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n📊 PIPELINE SUMMARY\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n🔹 Stage 1: Contrastive Pretraining (Self-Supervised)\")\n",
    "print(f\"   - Training samples: {len(pretrain_datasets['train_dataset'])}\")\n",
    "print(f\"   - Epochs: {PRETRAIN_EPOCHS}\")\n",
    "print(f\"   - Method: Contrastive learning on temporal windows\")\n",
    "print(f\"   - Result: Pretrained encoder with learned fMRI representations\")\n",
    "\n",
    "print(\"\\n🔹 Stage 2: Supervised Fine-tuning\")\n",
    "print(f\"   - Training samples: {len(finetune_datasets['train_dataset'])}\")\n",
    "print(f\"   - Validation samples: {len(finetune_datasets['val_dataset'])}\")\n",
    "print(f\"   - Test samples: {len(finetune_datasets['test_dataset'])}\")\n",
    "print(f\"   - Epochs: {FINETUNE_EPOCHS}\")\n",
    "print(f\"   - Task: {DEGRADATION_TASKS[0]}\")\n",
    "\n",
    "print(\"\\n🎯 FINAL RESULTS\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"   Test Accuracy:            {test_acc:.2f}%\")\n",
    "print(f\"   Test Precision:           {precision:.4f}\")\n",
    "print(f\"   Test Recall:              {recall:.4f}\")\n",
    "print(f\"   Test F1-Score:            {f1:.4f}\")\n",
    "\n",
    "print(\"\\n💾 SAVED MODELS\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   Pretrained encoder: {pretrained_path}\")\n",
    "print(f\"   Fine-tuned model:   {final_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ SwiFT pipeline successfully completed!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcb13cc",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "### For Better Performance:\n",
    "1. **Increase training epochs**:\n",
    "   - Pretraining: 100-300 epochs\n",
    "   - Fine-tuning: 50-100 epochs\n",
    "\n",
    "2. **Hyperparameter tuning**:\n",
    "   - Learning rates\n",
    "   - Temperature for contrastive loss\n",
    "   - Batch size\n",
    "\n",
    "3. **Data augmentation**:\n",
    "   - Add spatial and temporal augmentations\n",
    "   - Helps with generalization\n",
    "\n",
    "4. **Multi-task learning**:\n",
    "   - Train on multiple degradation horizons simultaneously\n",
    "   - Can improve overall performance\n",
    "\n",
    "### Using the Trained Model:\n",
    "\n",
    "```python\n",
    "# Load the fine-tuned model\n",
    "checkpoint = torch.load('../checkpoints/finetuned_degradation.pth')\n",
    "\n",
    "# Initialize models\n",
    "encoder = SwinTransformer4D(**checkpoint['config']['model_config'])\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "\n",
    "head = ClassificationHead(num_classes=2, num_features=288)\n",
    "head.load_state_dict(checkpoint['head_state_dict'])\n",
    "\n",
    "# Make predictions\n",
    "encoder.eval()\n",
    "head.eval()\n",
    "with torch.no_grad():\n",
    "    features = encoder(new_data)\n",
    "    predictions = torch.sigmoid(head(features))\n",
    "```\n",
    "\n",
    "### Documentation:\n",
    "- Full documentation: `data/README_ADNI.md`\n",
    "- Dataset classes: `data/dataset_adni.py`\n",
    "- Data preparation: `data/prepare_adni_data.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py40",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
