{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd9147ed",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0171ec5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful!\n",
      "PyTorch version: 2.0.1+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Import pipeline modules\n",
    "from data.preprocessing import preprocess_scan\n",
    "from data.dataset import SwiFTPretrainDataset, SwiFTFinetuneDataset, create_dummy_labels\n",
    "from models.swin4d_transformer_ver7 import SwinTransformer4D\n",
    "from models.heads import ContrastiveHead, ClassificationHead\n",
    "from training.losses import NTXentLoss\n",
    "from configs.config_pretrain import (\n",
    "    MODEL_CONFIG,\n",
    "    CONTRASTIVE_CONFIG,\n",
    "    TRAIN_CONFIG,\n",
    "    DATA_CONFIG,\n",
    ")\n",
    "\n",
    "print(\"✓ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55589ae7",
   "metadata": {},
   "source": [
    "## TEST 1: Data Preprocessing\n",
    "\n",
    "Test the data preprocessing pipeline with a dummy scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a5770fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST 1: Data Preprocessing\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TEST 1: Data Preprocessing\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "92ff6feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dummy scan with shape [1, 91, 109, 91, 140]...\n",
      "✓ Created dummy scan: torch.Size([1, 91, 109, 91, 140])\n",
      "  - Simulated brain region: H[10:85], W[15:95], D[10:85]\n",
      "✓ Created dummy scan: torch.Size([1, 91, 109, 91, 140])\n",
      "  - Simulated brain region: H[10:85], W[15:95], D[10:85]\n"
     ]
    }
   ],
   "source": [
    "# Create dummy scan [1, 91, 109, 91, 140]\n",
    "print(\"Creating dummy scan with shape [1, 91, 109, 91, 140]...\")\n",
    "dummy_scan = torch.randn(1, 91, 109, 91, 140)\n",
    "\n",
    "# Add some \"brain-like\" structure (non-zero regions in center)\n",
    "# This simulates brain regions with higher values than background\n",
    "dummy_scan[:, 10:85, 15:95, 10:85, :] += 5.0\n",
    "\n",
    "print(f\"✓ Created dummy scan: {dummy_scan.shape}\")\n",
    "print(f\"  - Simulated brain region: H[10:85], W[15:95], D[10:85]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4e01ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running preprocessing pipeline...\n",
      "Input shape: torch.Size([1, 91, 109, 91, 140])\n",
      "After adding channel: torch.Size([1, 1, 91, 109, 91, 140])\n",
      "Detected brain regions: 91x109x91\n",
      "Background value: 0.000001\n",
      "  Cropped to brain regions: H[0:91]=91, W[0:109]=109, D[0:91]=91\n",
      "After brain-aware cropping: torch.Size([91, 109, 91])\n",
      "After normalization - background value updated to: -2.948255\n",
      "After windowing: torch.Size([13, 1, 91, 109, 91, 20]), 13 windows\n",
      "\n",
      "✓ Preprocessing complete!\n",
      "  - Output shape: torch.Size([13, 1, 91, 109, 91, 20])\n",
      "  - Number of windows: 13\n",
      "  - Background value: -2.948255\n",
      "  - Brain bbox: {'height': (0, 91), 'width': (0, 109), 'depth': (0, 91)}\n",
      "  - Current spatial size: torch.Size([91, 109, 91])\n",
      "  - Needs padding: (5, -13, 5)\n",
      "After normalization - background value updated to: -2.948255\n",
      "After windowing: torch.Size([13, 1, 91, 109, 91, 20]), 13 windows\n",
      "\n",
      "✓ Preprocessing complete!\n",
      "  - Output shape: torch.Size([13, 1, 91, 109, 91, 20])\n",
      "  - Number of windows: 13\n",
      "  - Background value: -2.948255\n",
      "  - Brain bbox: {'height': (0, 91), 'width': (0, 109), 'depth': (0, 91)}\n",
      "  - Current spatial size: torch.Size([91, 109, 91])\n",
      "  - Needs padding: (5, -13, 5)\n",
      "  - Data range: [-2.948, 2.978]\n",
      "  - Data stats: mean=-0.000, std=1.000\n",
      "\n",
      "ℹ️  Temporal windowing explanation:\n",
      "  - Original time points: 140\n",
      "  - Window size: 20\n",
      "  - Stride: 10\n",
      "  - Windows overlap: YES (stride < window_size)\n",
      "  - Number of windows: (140 - 20) / 10 + 1 = 13\n",
      "  - Formula: (T - window_size) / stride + 1\n",
      "  - Data range: [-2.948, 2.978]\n",
      "  - Data stats: mean=-0.000, std=1.000\n",
      "\n",
      "ℹ️  Temporal windowing explanation:\n",
      "  - Original time points: 140\n",
      "  - Window size: 20\n",
      "  - Stride: 10\n",
      "  - Windows overlap: YES (stride < window_size)\n",
      "  - Number of windows: (140 - 20) / 10 + 1 = 13\n",
      "  - Formula: (T - window_size) / stride + 1\n"
     ]
    }
   ],
   "source": [
    "# Preprocess (NOW returns 3 values: data, indices, metadata)\n",
    "print(\"Running preprocessing pipeline...\")\n",
    "preprocessed, indices, metadata = preprocess_scan(\n",
    "    dummy_scan,\n",
    "    target_spatial_size=DATA_CONFIG[\"target_spatial_size\"],\n",
    "    window_size=DATA_CONFIG[\"window_size\"],\n",
    "    stride=DATA_CONFIG[\"stride\"],\n",
    "    normalize=DATA_CONFIG[\"normalize\"],\n",
    "    to_float16=False,  # Keep float32 for validation\n",
    "    crop_background=True,  # Use brain-aware cropping\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Preprocessing complete!\")\n",
    "print(f\"  - Output shape: {preprocessed.shape}\")\n",
    "print(f\"  - Number of windows: {len(indices)}\")\n",
    "print(f\"  - Background value: {metadata['background_value']:.6f}\")\n",
    "print(f\"  - Brain bbox: {metadata['brain_bbox']}\")\n",
    "print(f\"  - Current spatial size: {metadata['current_spatial_size']}\")\n",
    "print(f\"  - Needs padding: {metadata['needs_padding']}\")\n",
    "print(f\"  - Data range: [{preprocessed.min():.3f}, {preprocessed.max():.3f}]\")\n",
    "print(f\"  - Data stats: mean={preprocessed.mean():.3f}, std={preprocessed.std():.3f}\")\n",
    "\n",
    "print(f\"\\nℹ️  Temporal windowing explanation:\")\n",
    "print(f\"  - Original time points: 140\")\n",
    "print(f\"  - Window size: {DATA_CONFIG['window_size']}\")\n",
    "print(f\"  - Stride: {DATA_CONFIG['stride']}\")\n",
    "print(f\"  - Windows overlap: YES (stride < window_size)\")\n",
    "print(f\"  - Number of windows: (140 - 20) / 10 + 1 = {len(indices)}\")\n",
    "print(f\"  - Formula: (T - window_size) / stride + 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be4fdd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Basic shape assertions passed!\n",
      "  Note: Spatial dimensions are torch.Size([91, 109, 91])\n",
      "  Final padding to 96x96x96 will happen in Dataset class\n"
     ]
    }
   ],
   "source": [
    "# Verify output shape\n",
    "# Note: Spatial dimensions may NOT be exactly 96x96x96 yet (padding happens in Dataset)\n",
    "assert preprocessed.shape[1] == 1, f\"Expected 1 channel, got {preprocessed.shape[1]}\"\n",
    "assert preprocessed.shape[-1] == 20, f\"Expected 20 time points, got {preprocessed.shape[-1]}\"\n",
    "assert len(indices) > 0, \"No windows created!\"\n",
    "\n",
    "print(\"✓ Basic shape assertions passed!\")\n",
    "print(f\"  Note: Spatial dimensions are {preprocessed.shape[2:5]}\")\n",
    "print(f\"  Final padding to 96x96x96 will happen in Dataset class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfdeb101",
   "metadata": {},
   "source": [
    "### Understanding Temporal Windows\n",
    "\n",
    "**Why overlapping windows?**\n",
    "\n",
    "SwiFT uses overlapping temporal windows to:\n",
    "1. **Increase training samples**: Get more data from limited scans\n",
    "2. **Capture temporal dynamics**: Overlapping windows help model learn smooth transitions\n",
    "3. **Standard practice**: Most video/time-series models use overlapping windows\n",
    "\n",
    "**Example with your data:**\n",
    "- Original: 140 time points\n",
    "- Window size: 20 time points\n",
    "- Stride: 10 time points\n",
    "\n",
    "Windows created:\n",
    "- Window 0: frames [0-19]\n",
    "- Window 1: frames [10-29] ← overlaps with Window 0 by 10 frames\n",
    "- Window 2: frames [20-39] ← overlaps with Window 1 by 10 frames\n",
    "- ...\n",
    "- Window 12: frames [120-139]\n",
    "\n",
    "Total: (140 - 20) / 10 + 1 = 13 windows ✓"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24901107",
   "metadata": {},
   "source": [
    "## TEST 2: Model Forward Pass\n",
    "\n",
    "Test the SwiFT model with a forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4367f3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST 2: Model Forward Pass\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TEST 2: Model Forward Pass\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b8fcea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing SwiFT model...\n",
      "img_size:  (96, 96, 96, 20)\n",
      "patch_size:  (6, 6, 6, 1)\n",
      "patch_dim:  (16, 16, 16, 20)\n",
      "✓ Model created\n",
      "  - Total parameters: 4,315,212\n",
      "  - Trainable parameters: 4,315,212\n",
      "✓ Model created\n",
      "  - Total parameters: 4,315,212\n",
      "  - Trainable parameters: 4,315,212\n"
     ]
    }
   ],
   "source": [
    "# Create model\n",
    "print(\"Initializing SwiFT model...\")\n",
    "model = SwinTransformer4D(**MODEL_CONFIG)\n",
    "print(f\"✓ Model created\")\n",
    "\n",
    "# Print model architecture summary\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"  - Total parameters: {total_params:,}\")\n",
    "print(f\"  - Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39d60b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing forward pass with input shape: torch.Size([2, 1, 96, 96, 96, 20])\n"
     ]
    }
   ],
   "source": [
    "# Create dummy input\n",
    "batch_size = 2\n",
    "dummy_input = torch.randn(batch_size, 1, 96, 96, 96, 20)\n",
    "print(f\"Testing forward pass with input shape: {dummy_input.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "666a31d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Forward pass successful!\n",
      "  - Output shape: torch.Size([2, 288, 2, 2, 2, 20])\n",
      "  - Expected feature dim: 288\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(dummy_input)\n",
    "\n",
    "print(f\"✓ Forward pass successful!\")\n",
    "print(f\"  - Output shape: {output.shape}\")\n",
    "expected_dim = MODEL_CONFIG['embed_dim'] * (MODEL_CONFIG['c_multiplier'] ** 3)\n",
    "print(f\"  - Expected feature dim: {expected_dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c135d532",
   "metadata": {},
   "source": [
    "## TEST 3: Contrastive Pretraining\n",
    "\n",
    "Pretrain the model using contrastive learning (self-supervised, no labels needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f543565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TEST 3: Contrastive Pretraining\n",
      "================================================================================\n",
      "Stage 1: Pretrain backbone with contrastive learning (unlabeled data)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TEST 3: Contrastive Pretraining\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"Stage 1: Pretrain backbone with contrastive learning (unlabeled data)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f99fc0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dummy data: torch.Size([12, 1, 90, 92, 88, 20])\n",
      "Created indices: torch.Size([12, 2])\n",
      "Background value for padding: -2.5\n"
     ]
    }
   ],
   "source": [
    "# Create dummy preprocessed data (simulating output after brain-aware cropping)\n",
    "num_windows = 12\n",
    "# Note: Using slightly smaller spatial dims to simulate brain-cropped data\n",
    "data = torch.randn(num_windows, 1, 90, 92, 88, 20)\n",
    "indices = torch.tensor([[0, i * 10] for i in range(num_windows)])\n",
    "\n",
    "# Create metadata (simulating preprocessing output)\n",
    "metadata = {\n",
    "    'background_value': -2.5,\n",
    "    'brain_bbox': {'height': (5, 85), 'width': (8, 100), 'depth': (5, 83)},\n",
    "    'needs_padding': (6, 4, 8),\n",
    "    'current_spatial_size': (90, 92, 88)\n",
    "}\n",
    "\n",
    "print(f\"Created dummy data: {data.shape}\")\n",
    "print(f\"Created indices: {indices.shape}\")\n",
    "print(f\"Background value for padding: {metadata['background_value']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94437c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating contrastive dataset...\n",
      "  Dataset will pad from torch.Size([90, 92, 88]) to (96, 96, 96)\n",
      "  Using background value: -2.500000\n",
      "Contrastive dataset: 12 valid samples from 1 scans\n",
      "✓ Dataset created: 12 samples\n",
      "  - Batch size: 4\n",
      "  - Padding will be applied during data loading\n"
     ]
    }
   ],
   "source": [
    "# Create contrastive dataset (NOW with metadata for padding)\n",
    "print(\"Creating contrastive dataset...\")\n",
    "dataset = SwiFTPretrainDataset(\n",
    "    data, \n",
    "    indices,\n",
    "    metadata=metadata,  # Pass metadata for background-value padding\n",
    "    target_spatial_size=(96, 96, 96)\n",
    ")\n",
    "dataloader = DataLoader(\n",
    "    dataset, batch_size=TRAIN_CONFIG[\"batch_size\"], shuffle=True\n",
    ")\n",
    "print(f\"✓ Dataset created: {len(dataset)} samples\")\n",
    "print(f\"  - Batch size: {TRAIN_CONFIG['batch_size']}\")\n",
    "print(f\"  - Padding will be applied during data loading\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e6c91e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model and contrastive head...\n",
      "img_size:  (96, 96, 96, 20)\n",
      "patch_size:  (6, 6, 6, 1)\n",
      "patch_dim:  (16, 16, 16, 20)\n",
      "✓ Models initialized on cpu\n",
      "✓ Models initialized on cpu\n"
     ]
    }
   ],
   "source": [
    "# Create model and contrastive head\n",
    "print(\"Initializing model and contrastive head...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "contrastive_model = SwinTransformer4D(**MODEL_CONFIG).to(device)\n",
    "contrastive_head = ContrastiveHead(**CONTRASTIVE_CONFIG).to(device)\n",
    "print(f\"✓ Models initialized on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea28b8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loss and optimizer created\n",
      "  - Learning rate: 5e-05\n",
      "  - Temperature: 0.5\n"
     ]
    }
   ],
   "source": [
    "# Create loss and optimizer\n",
    "criterion = NTXentLoss(\n",
    "    device=device,\n",
    "    batch_size=TRAIN_CONFIG[\"batch_size\"],\n",
    "    temperature=TRAIN_CONFIG[\"temperature\"],\n",
    "    use_cosine_similarity=TRAIN_CONFIG[\"use_cosine_similarity\"],\n",
    ")\n",
    "optimizer = optim.AdamW(\n",
    "    list(contrastive_model.parameters()) + list(contrastive_head.parameters()),\n",
    "    lr=TRAIN_CONFIG[\"learning_rate\"],\n",
    ")\n",
    "print(f\"✓ Loss and optimizer created\")\n",
    "print(f\"  - Learning rate: {TRAIN_CONFIG['learning_rate']}\")\n",
    "print(f\"  - Temperature: {TRAIN_CONFIG['temperature']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6e34389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pretraining (5 epochs)...\n",
      "Note: In real training, you'd use many more epochs (e.g., 300 in SwiFT paper)\n",
      "  ✓ Shapes verified: both views are 96x96x96 after padding\n",
      "  ✓ Shapes verified: both views are 96x96x96 after padding\n",
      "  Epoch 1/5: Loss = 1.9452\n",
      "  Epoch 1/5: Loss = 1.9452\n",
      "  Epoch 2/5: Loss = 1.9170\n",
      "  Epoch 2/5: Loss = 1.9170\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 35\u001b[0m\n\u001b[0;32m     32\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(embeddings1, embeddings2)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     38\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\Y\\envs\\py39\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\Y\\envs\\py39\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run pretraining iterations (simulate multiple epochs)\n",
    "print(\"Running pretraining (5 epochs)...\")\n",
    "print(\"Note: In real training, you'd use many more epochs (e.g., 300 in SwiFT paper)\")\n",
    "contrastive_model.train()\n",
    "contrastive_head.train()\n",
    "\n",
    "for epoch in range(5):\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, (view1, view2) in enumerate(dataloader):\n",
    "        view1, view2 = view1.to(device), view2.to(device)\n",
    "        \n",
    "        # Verify shapes after padding (only first batch)\n",
    "        if batch_idx == 0 and epoch == 0:\n",
    "            assert view1.shape[2:5] == (96, 96, 96), f\"Expected 96x96x96, got {view1.shape[2:5]}\"\n",
    "            assert view2.shape[2:5] == (96, 96, 96), f\"Expected 96x96x96, got {view2.shape[2:5]}\"\n",
    "            print(f\"  ✓ Shapes verified: both views are 96x96x96 after padding\")\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Encode both views\n",
    "        features1 = contrastive_model(view1)\n",
    "        features2 = contrastive_model(view2)\n",
    "\n",
    "        # Project to embedding space\n",
    "        embeddings1 = contrastive_head(features1)\n",
    "        embeddings2 = contrastive_head(features2)\n",
    "\n",
    "        # Compute contrastive loss\n",
    "        loss = criterion(embeddings1, embeddings2)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        if num_batches >= 2:  # Limit batches for demo\n",
    "            break\n",
    "    \n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    print(f\"  Epoch {epoch+1}/5: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "print(f\"\\n✓ Pretraining complete!\")\n",
    "print(\"Backbone has learned representations from unlabeled data using contrastive learning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a03b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save pretrained backbone weights\n",
    "print(\"\\nSaving pretrained weights...\")\n",
    "pretrained_path = \"../checkpoints/pretrained_backbone.pth\"\n",
    "os.makedirs(os.path.dirname(pretrained_path), exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': contrastive_model.state_dict(),\n",
    "    'config': MODEL_CONFIG,\n",
    "    'training_info': {\n",
    "        'method': 'contrastive_pretraining',\n",
    "        'final_loss': loss.item(),\n",
    "    }\n",
    "}, pretrained_path)\n",
    "\n",
    "print(f\"✓ Saved pretrained weights to: {pretrained_path}\")\n",
    "print(\"Note: Only the backbone is saved, not the contrastive projection head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea63a43",
   "metadata": {},
   "source": [
    "## TEST 4: Fine-tuning for Classification\n",
    "\n",
    "Use the pretrained backbone to fine-tune on a supervised classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdae830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"TEST 4: Fine-tuning for Classification\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"Stage 2: Fine-tune on supervised task using pretrained features\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4b68e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import finetuning config\n",
    "from configs.config_finetune import (\n",
    "    TASK_CONFIG,\n",
    "    HEAD_CONFIG,\n",
    "    TRAIN_CONFIG as FINETUNE_TRAIN_CONFIG,\n",
    "    OPTIMIZER_CONFIG,\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning configuration:\")\n",
    "print(f\"  - Task type: {TASK_CONFIG['task_type']}\")\n",
    "print(f\"  - Number of classes: {TASK_CONFIG['num_classes']}\")\n",
    "print(f\"  - Freeze encoder: {TASK_CONFIG['freeze_encoder']}\")\n",
    "print(f\"  - Learning rate: {FINETUNE_TRAIN_CONFIG['learning_rate']}\")\n",
    "print(f\"  - Batch size: {FINETUNE_TRAIN_CONFIG['batch_size']}\")\n",
    "print(f\"  - Epochs: {FINETUNE_TRAIN_CONFIG['num_epochs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660d8efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained backbone\n",
    "print(\"Loading pretrained backbone weights...\")\n",
    "finetuned_model = SwinTransformer4D(**MODEL_CONFIG).to(device)\n",
    "\n",
    "# Load weights from saved checkpoint\n",
    "checkpoint = torch.load(pretrained_path, map_location=device)\n",
    "finetuned_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"✓ Loaded pretrained weights from: {pretrained_path}\")\n",
    "\n",
    "# Freeze encoder weights if specified\n",
    "if TASK_CONFIG['freeze_encoder']:\n",
    "    print(\"Freezing encoder weights...\")\n",
    "    for param in finetuned_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    trainable = sum(p.numel() for p in finetuned_model.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in finetuned_model.parameters())\n",
    "    print(f\"  - Trainable parameters: {trainable:,} / {total:,}\")\n",
    "else:\n",
    "    print(\"Encoder weights are trainable (not frozen)\")\n",
    "    trainable = sum(p.numel() for p in finetuned_model.parameters() if p.requires_grad)\n",
    "    print(f\"  - Trainable parameters: {trainable:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4cbd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create classification head\n",
    "print(\"Creating classification head...\")\n",
    "classification_head = ClassificationHead(\n",
    "    num_classes=HEAD_CONFIG['num_classes'],\n",
    "    num_features=HEAD_CONFIG['num_features']\n",
    ").to(device)\n",
    "\n",
    "head_params = sum(p.numel() for p in classification_head.parameters())\n",
    "print(f\"✓ Classification head created\")\n",
    "print(f\"  - Head parameters: {head_params:,}\")\n",
    "print(f\"  - Input features: {HEAD_CONFIG['num_features']}\")\n",
    "print(f\"  - Output classes: {HEAD_CONFIG['num_classes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c11c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create finetuning dataset with labels\n",
    "print(\"Creating finetuning dataset with labels...\")\n",
    "\n",
    "# Create dummy binary classification labels\n",
    "num_samples = len(data)\n",
    "labels = create_dummy_labels(num_samples, task_type='binary')\n",
    "print(f\"  - Created {num_samples} labels\")\n",
    "print(f\"  - Label distribution: {labels.sum().item()} positive, {(labels == 0).sum().item()} negative\")\n",
    "\n",
    "# Split into train/val (80/20)\n",
    "train_size = int(0.8 * num_samples)\n",
    "train_data = data[:train_size]\n",
    "train_labels = labels[:train_size]\n",
    "val_data = data[train_size:]\n",
    "val_labels = labels[train_size:]\n",
    "\n",
    "print(f\"  - Train samples: {len(train_data)}\")\n",
    "print(f\"  - Val samples: {len(val_data)}\")\n",
    "\n",
    "# Create datasets (use same metadata for padding)\n",
    "train_dataset = SwiFTFinetuneDataset(\n",
    "    train_data, \n",
    "    train_labels,\n",
    "    metadata=metadata,\n",
    "    target_spatial_size=(96, 96, 96)\n",
    ")\n",
    "\n",
    "val_dataset = SwiFTFinetuneDataset(\n",
    "    val_data, \n",
    "    val_labels,\n",
    "    metadata=metadata,\n",
    "    target_spatial_size=(96, 96, 96)\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=FINETUNE_TRAIN_CONFIG['batch_size'], \n",
    "    shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=FINETUNE_TRAIN_CONFIG['batch_size'], \n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"✓ Datasets and dataloaders created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe77e623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer and loss function\n",
    "print(\"Setting up optimizer and loss function...\")\n",
    "\n",
    "# Only optimize classification head parameters (encoder is frozen)\n",
    "if TASK_CONFIG['freeze_encoder']:\n",
    "    params_to_optimize = classification_head.parameters()\n",
    "else:\n",
    "    params_to_optimize = list(finetuned_model.parameters()) + list(classification_head.parameters())\n",
    "\n",
    "finetune_optimizer = optim.AdamW(\n",
    "    params_to_optimize,\n",
    "    lr=OPTIMIZER_CONFIG['lr'],\n",
    "    weight_decay=OPTIMIZER_CONFIG['weight_decay'],\n",
    "    betas=OPTIMIZER_CONFIG['betas']\n",
    ")\n",
    "\n",
    "# Binary classification loss\n",
    "finetune_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(f\"✓ Optimizer and loss function created\")\n",
    "print(f\"  - Optimizer: AdamW\")\n",
    "print(f\"  - Learning rate: {OPTIMIZER_CONFIG['lr']}\")\n",
    "print(f\"  - Loss function: BCEWithLogitsLoss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac200e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for fine-tuning\n",
    "print(\"Running fine-tuning training (10 epochs for demo)...\")\n",
    "print(\"Note: In real training, you'd use more epochs (e.g., 50-100)\")\n",
    "\n",
    "num_epochs = 10\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# Set model to appropriate mode\n",
    "finetuned_model.eval()  # Keep frozen encoder in eval mode\n",
    "classification_head.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.float().to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        finetune_optimizer.zero_grad()\n",
    "        \n",
    "        # Get features from pretrained encoder (no gradients if frozen)\n",
    "        with torch.set_grad_enabled(not TASK_CONFIG['freeze_encoder']):\n",
    "            features = finetuned_model(inputs)\n",
    "        \n",
    "        # Get predictions from classification head\n",
    "        outputs = classification_head(features)\n",
    "        outputs = outputs.squeeze()\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = finetune_criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass (only for classification head if encoder is frozen)\n",
    "        loss.backward()\n",
    "        finetune_optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        train_loss += loss.item()\n",
    "        predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        train_correct += (predictions == targets).sum().item()\n",
    "        train_total += targets.size(0)\n",
    "    \n",
    "    # Validation phase\n",
    "    classification_head.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.float().to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            features = finetuned_model(inputs)\n",
    "            outputs = classification_head(features)\n",
    "            outputs = outputs.squeeze()\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = finetune_criterion(outputs, targets)\n",
    "            \n",
    "            # Track metrics\n",
    "            val_loss += loss.item()\n",
    "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            val_correct += (predictions == targets).sum().item()\n",
    "            val_total += targets.size(0)\n",
    "    \n",
    "    # Calculate epoch metrics\n",
    "    train_loss /= len(train_loader)\n",
    "    train_acc = 100 * train_correct / train_total\n",
    "    val_loss /= len(val_loader)\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    \n",
    "    print(f\"  Epoch {epoch+1}/{num_epochs}: \"\n",
    "          f\"Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}%, \"\n",
    "          f\"Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_epoch = epoch + 1\n",
    "    \n",
    "    # Set back to train mode for next epoch\n",
    "    classification_head.train()\n",
    "\n",
    "print(f\"\\n✓ Fine-tuning complete!\")\n",
    "print(f\"  Best validation loss: {best_val_loss:.4f} at epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de26a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save finetuned model\n",
    "print(\"\\nSaving finetuned model...\")\n",
    "finetuned_checkpoint_path = \"../checkpoints/finetuned_model.pth\"\n",
    "os.makedirs(os.path.dirname(finetuned_checkpoint_path), exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'encoder_state_dict': finetuned_model.state_dict(),\n",
    "    'head_state_dict': classification_head.state_dict(),\n",
    "    'optimizer_state_dict': finetune_optimizer.state_dict(),\n",
    "    'config': {\n",
    "        'model_config': MODEL_CONFIG,\n",
    "        'task_config': TASK_CONFIG,\n",
    "        'head_config': HEAD_CONFIG,\n",
    "    },\n",
    "    'training_info': {\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_epoch': best_epoch,\n",
    "        'final_train_loss': train_loss,\n",
    "        'final_val_loss': val_loss,\n",
    "    }\n",
    "}, finetuned_checkpoint_path)\n",
    "\n",
    "print(f\"✓ Saved finetuned model to: {finetuned_checkpoint_path}\")\n",
    "print(\"  - Saved encoder (backbone) weights\")\n",
    "print(\"  - Saved classification head weights\")\n",
    "print(\"  - Saved optimizer state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2556b42",
   "metadata": {},
   "source": [
    "## Summary: Complete SwiFT Pipeline\n",
    "\n",
    "The notebook demonstrates the complete SwiFT workflow:\n",
    "\n",
    "### Stage 1: Contrastive Pretraining (Self-Supervised)\n",
    "- **Goal**: Learn robust representations from unlabeled fMRI data\n",
    "- **Method**: Contrastive learning with temporal augmentation\n",
    "- **Output**: Pretrained encoder that understands fMRI patterns\n",
    "- **Key advantage**: No labels needed, can use large amounts of unlabeled data\n",
    "\n",
    "### Stage 2: Supervised Fine-tuning\n",
    "- **Goal**: Adapt pretrained encoder to specific downstream task\n",
    "- **Method**: Freeze encoder, train lightweight classification head\n",
    "- **Output**: Task-specific classifier with strong performance\n",
    "- **Key advantage**: Requires fewer labeled samples, faster training\n",
    "\n",
    "### Benefits of this approach:\n",
    "1. **Data efficiency**: Pretrain on large unlabeled dataset, fine-tune on small labeled dataset\n",
    "2. **Transfer learning**: Learned features generalize across tasks\n",
    "3. **Better performance**: Pretraining provides better initialization than random weights\n",
    "4. **Faster convergence**: Fine-tuning converges faster than training from scratch\n",
    "\n",
    "This is the standard approach used in many successful deep learning applications (e.g., BERT for NLP, SwiFT for fMRI)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
