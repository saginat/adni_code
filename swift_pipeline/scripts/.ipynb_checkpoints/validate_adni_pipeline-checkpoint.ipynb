{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d48d0354",
   "metadata": {},
   "source": [
    "# SwiFT Pipeline with ADNI Degradation Data\n",
    "\n",
    "This notebook demonstrates the complete SwiFT (Swin 4D fMRI Transformer) pipeline applied to ADNI (Alzheimer's Disease Neuroimaging Initiative) data for cognitive decline prediction.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The notebook covers:\n",
    "1. **Setup and Configuration** - Import libraries and configure paths\n",
    "2. **Stage 1: Contrastive Pretraining** - Self-supervised learning on unlabeled fMRI data\n",
    "3. **Stage 2: Supervised Fine-tuning** - Training on degradation prediction tasks\n",
    "4. **Evaluation** - Test set evaluation and metrics\n",
    "\n",
    "## SwiFT Methodology\n",
    "\n",
    "SwiFT uses a two-stage approach:\n",
    "- **Pretraining**: Learn robust fMRI representations using contrastive learning (no labels needed)\n",
    "- **Fine-tuning**: Adapt the pretrained model to specific downstream tasks (degradation prediction)\n",
    "\n",
    "This approach allows efficient use of limited labeled data by leveraging unlabeled data during pretraining."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e072bf",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60dce67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Sagi\\Desktop\\fixed_code\\Y\\envs\\py39\\lib\\site-packages\\monai\\utils\\module.py:399: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  pkg = __import__(module)  # top level module\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SWIFT PIPELINE WITH ADNI DEGRADATION DATA\n",
      "================================================================================\n",
      "\n",
      "✓ All imports successful!\n",
      "PyTorch version: 2.0.1+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# SwiFT pipeline imports\n",
    "from data.prepare_adni_data import prepare_adni_datasets, create_adni_dataloaders\n",
    "from models.swin4d_transformer_ver7 import SwinTransformer4D\n",
    "from models.heads import ContrastiveHead, ClassificationHead\n",
    "from training.losses import NTXentLoss\n",
    "from configs.config_pretrain import MODEL_CONFIG, CONTRASTIVE_CONFIG, TRAIN_CONFIG, DATA_CONFIG\n",
    "from configs.config_finetune import TASK_CONFIG, HEAD_CONFIG, TRAIN_CONFIG as FINETUNE_TRAIN_CONFIG\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SWIFT PIPELINE WITH ADNI DEGRADATION DATA\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n✓ All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0043c4",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Set up paths to your ADNI data files and training parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf85470",
   "metadata": {},
   "source": [
    "### 1.1 (Optional) Generate Dummy Data\n",
    "\n",
    "If you don't have access to real ADNI data, run this cell to generate synthetic test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07091296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dummy data already exists!\n",
      "  Location: ../data_dummy/all_4d_downsampled.pt\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to generate dummy ADNI data for testing\n",
    "# This creates synthetic tensors and JSON files that mimic real ADNI data\n",
    "\n",
    "# Check if dummy data already exists\n",
    "import os\n",
    "dummy_data_path = \"../data_dummy/all_4d_downsampled.pt\"\n",
    "\n",
    "if os.path.exists(dummy_data_path):\n",
    "    print(\"✓ Dummy data already exists!\")\n",
    "    print(f\"  Location: {dummy_data_path}\")\n",
    "else:\n",
    "    print(\"Creating dummy ADNI data...\")\n",
    "    print(\"This will take a few seconds...\\n\")\n",
    "    \n",
    "    # Run the dummy data creation script\n",
    "    exec(open('create_dummy_adni_data.py').read())\n",
    "    \n",
    "    print(\"\\n✓ Dummy data creation complete!\")\n",
    "    print(\"  You can now proceed with the rest of the notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f47f9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "995a8eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  Using DUMMY data for testing\n",
      "   Run 'create_dummy_adni_data.py' first to generate dummy data\n",
      "Data path: ../data_dummy/all_4d_downsampled.pt\n",
      "\n",
      "Device: cpu\n",
      "Target spatial size: (96, 96, 96)\n",
      "Task(s): ['degradation_binary_1year']\n",
      "Pretraining epochs: 5\n",
      "Fine-tuning epochs: 10\n"
     ]
    }
   ],
   "source": [
    "# Paths to your ADNI data\n",
    "# OPTION 1: Use dummy data (for testing without real ADNI data)\n",
    "USE_DUMMY_DATA = True  # Set to False when using real ADNI data\n",
    "\n",
    "if USE_DUMMY_DATA:\n",
    "    BASE_DATA_PATH = \"../data_dummy\"\n",
    "    DATA_PATH = f\"{BASE_DATA_PATH}/all_4d_downsampled.pt\"\n",
    "    LABELS_PATH = f\"{BASE_DATA_PATH}/imageID_to_labels.json\"\n",
    "    INFO_PATH = f\"{BASE_DATA_PATH}/index_to_name.json\"\n",
    "    print(\"⚠️  Using DUMMY data for testing\")\n",
    "    print(\"   Run 'create_dummy_adni_data.py' first to generate dummy data\")\n",
    "else:\n",
    "    # OPTION 2: Use real ADNI data (adjust paths to your data location)\n",
    "    BASE_DATA_PATH = \"../../\"\n",
    "    DATA_PATH = f\"{BASE_DATA_PATH}/data/all_4d_downsampled.pt\"\n",
    "    LABELS_PATH = f\"{BASE_DATA_PATH}/imageID_to_labels.json\"\n",
    "    INFO_PATH = f\"{BASE_DATA_PATH}/index_to_name.json\"\n",
    "    print(\"✓ Using REAL ADNI data\")\n",
    "\n",
    "print(f\"Data path: {DATA_PATH}\")\n",
    "\n",
    "# Task configuration - choose which degradation task to predict\n",
    "DEGRADATION_TASKS = [\n",
    "    \"degradation_binary_1year\",\n",
    "    # \"degradation_binary_2years\",  # Uncomment for multi-task learning\n",
    "    # \"degradation_binary_3years\",\n",
    "]\n",
    "\n",
    "# Spatial configuration\n",
    "TARGET_SPATIAL_SIZE = (96, 96, 96)  # Model expects 96x96x96 input\n",
    "# You can change this to a lower resolution if needed, e.g., (64, 64, 64)\n",
    "# Note: The model must be configured with the same spatial size\n",
    "\n",
    "# Training configuration\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PRETRAIN_EPOCHS = 5  # Increase for real training (e.g., 100-300)\n",
    "FINETUNE_EPOCHS = 10  # Increase for real training (e.g., 50-100)\n",
    "\n",
    "print(f\"\\nDevice: {DEVICE}\")\n",
    "print(f\"Target spatial size: {TARGET_SPATIAL_SIZE}\")\n",
    "print(f\"Task(s): {DEGRADATION_TASKS}\")\n",
    "print(f\"Pretraining epochs: {PRETRAIN_EPOCHS}\")\n",
    "print(f\"Fine-tuning epochs: {FINETUNE_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7a45fb",
   "metadata": {},
   "source": [
    "## 3. Stage 1: Contrastive Pretraining (Self-Supervised)\n",
    "\n",
    "In this stage, we train the SwiFT encoder using contrastive learning **without labels**. The model learns to create meaningful representations of fMRI data by:\n",
    "- Taking two different temporal windows from the same scan\n",
    "- Learning to make their representations similar (positive pairs)\n",
    "- While making representations from different scans dissimilar (negative pairs)\n",
    "\n",
    "This is self-supervised learning - no degradation labels are needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c8d6e6",
   "metadata": {},
   "source": [
    "### 3.1 Prepare Pretraining Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1162603f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36fbbbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 1: CONTRASTIVE PRETRAINING\n",
      "================================================================================\n",
      "================================================================================\n",
      "PREPARING ADNI DATA FOR SWIFT PIPELINE\n",
      "================================================================================\n",
      "Loading ADNI data...\n",
      "  - Data shape: torch.Size([10, 91, 109, 91, 140])\n",
      "  - Loaded labels for 10 scans\n",
      "  - Loaded info for 10 samples\n",
      "\n",
      "Loaded data shape: torch.Size([10, 91, 109, 91, 140])\n",
      "Expected format: [N_scans, H, W, D, T]\n",
      "\n",
      "Creating temporal windows:\n",
      "  - Window size: 20\n",
      "  - Stride: 10\n",
      "  - Data shape: torch.Size([10, 91, 109, 91, 140])\n",
      "  - Loaded labels for 10 scans\n",
      "  - Loaded info for 10 samples\n",
      "\n",
      "Loaded data shape: torch.Size([10, 91, 109, 91, 140])\n",
      "Expected format: [N_scans, H, W, D, T]\n",
      "\n",
      "Creating temporal windows:\n",
      "  - Window size: 20\n",
      "  - Stride: 10\n",
      "✓ Created 130 windows from 10 scans\n",
      "  - Windowed data shape: torch.Size([130, 91, 109, 91, 20])\n",
      "  - Windows per scan: ~13\n",
      "\n",
      "Data split by subjects (no data leakage):\n",
      "  Train: 7 subjects, 91 samples\n",
      "  Val:   1 subjects, 13 samples\n",
      "  Test:  2 subjects, 26 samples\n",
      "✓ Created 130 windows from 10 scans\n",
      "  - Windowed data shape: torch.Size([130, 91, 109, 91, 20])\n",
      "  - Windows per scan: ~13\n",
      "\n",
      "Data split by subjects (no data leakage):\n",
      "  Train: 7 subjects, 91 samples\n",
      "  Val:   1 subjects, 13 samples\n",
      "  Test:  2 subjects, 26 samples\n",
      "\n",
      "Data shapes:\n",
      "  Train: torch.Size([91, 91, 109, 91, 20])\n",
      "  Val:   torch.Size([13, 91, 109, 91, 20])\n",
      "  Test:  torch.Size([26, 91, 109, 91, 20])\n",
      "\n",
      "================================================================================\n",
      "CREATING CONTRASTIVE PRETRAINING DATASETS\n",
      "================================================================================\n",
      "  Dataset will resize from torch.Size([91, 109, 91]) to (96, 96, 96)\n",
      "  Using background value: 0.000000\n",
      "Contrastive dataset: 91 valid samples from 7 scans\n",
      "  Dataset will resize from torch.Size([91, 109, 91]) to (96, 96, 96)\n",
      "  Using background value: 0.000000\n",
      "Contrastive dataset: 13 valid samples from 1 scans\n",
      "\n",
      "================================================================================\n",
      "DATA PREPARATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "\n",
      "Data shapes:\n",
      "  Train: torch.Size([91, 91, 109, 91, 20])\n",
      "  Val:   torch.Size([13, 91, 109, 91, 20])\n",
      "  Test:  torch.Size([26, 91, 109, 91, 20])\n",
      "\n",
      "================================================================================\n",
      "CREATING CONTRASTIVE PRETRAINING DATASETS\n",
      "================================================================================\n",
      "  Dataset will resize from torch.Size([91, 109, 91]) to (96, 96, 96)\n",
      "  Using background value: 0.000000\n",
      "Contrastive dataset: 91 valid samples from 7 scans\n",
      "  Dataset will resize from torch.Size([91, 109, 91]) to (96, 96, 96)\n",
      "  Using background value: 0.000000\n",
      "Contrastive dataset: 13 valid samples from 1 scans\n",
      "\n",
      "================================================================================\n",
      "DATA PREPARATION COMPLETE\n",
      "================================================================================\n",
      "\n",
      "\n",
      "✓ Dataloaders created:\n",
      "  - Train batches: 23\n",
      "  - Val batches: 4\n",
      "\n",
      "✓ Dataloaders created:\n",
      "  - Train batches: 23\n",
      "  - Val batches: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 1: CONTRASTIVE PRETRAINING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare datasets for pretraining\n",
    "pretrain_datasets = prepare_adni_datasets(\n",
    "    data_path=DATA_PATH,\n",
    "    labels_path=LABELS_PATH,\n",
    "    info_path=INFO_PATH,\n",
    "    task_names=DEGRADATION_TASKS,  # Not used in pretraining\n",
    "    stage=\"pretrain\",\n",
    "    val_split=0.1,\n",
    "    test_split=0.2,\n",
    "    seed=42,\n",
    "    target_spatial_size=TARGET_SPATIAL_SIZE,\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "pretrain_loaders = create_adni_dataloaders(\n",
    "    pretrain_datasets,\n",
    "    batch_size=TRAIN_CONFIG[\"batch_size\"],\n",
    "    shuffle_train=True,\n",
    ")\n",
    "\n",
    "train_loader = pretrain_loaders[\"train_loader\"]\n",
    "val_loader = pretrain_loaders[\"val_loader\"]\n",
    "\n",
    "print(f\"\\n✓ Dataloaders created:\")\n",
    "print(f\"  - Train batches: {len(train_loader)}\")\n",
    "print(f\"  - Val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff221f56",
   "metadata": {},
   "source": [
    "### 3.2 Initialize Models for Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6fec02f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing models for pretraining...\n",
      "img_size:  (96, 96, 96, 20)\n",
      "patch_size:  (6, 6, 6, 1)\n",
      "patch_dim:  (16, 16, 16, 20)\n",
      "\n",
      "✓ Models initialized on cpu\n",
      "  - Encoder parameters: 4,315,212\n",
      "  - Contrastive head parameters: 120,800\n",
      "✓ Loss function: NTXentLoss (temperature=0.5)\n",
      "✓ Optimizer: AdamW (lr=5e-05)\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing models for pretraining...\")\n",
    "\n",
    "# SwiFT Encoder (4D Swin Transformer)\n",
    "encoder = SwinTransformer4D(**MODEL_CONFIG).to(DEVICE)\n",
    "\n",
    "# Contrastive projection head\n",
    "contrastive_head = ContrastiveHead(**CONTRASTIVE_CONFIG).to(DEVICE)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in encoder.parameters())\n",
    "head_params = sum(p.numel() for p in contrastive_head.parameters())\n",
    "print(f\"\\n✓ Models initialized on {DEVICE}\")\n",
    "print(f\"  - Encoder parameters: {total_params:,}\")\n",
    "print(f\"  - Contrastive head parameters: {head_params:,}\")\n",
    "\n",
    "# Setup training\n",
    "criterion = NTXentLoss(\n",
    "    device=DEVICE,\n",
    "    batch_size=TRAIN_CONFIG[\"batch_size\"],\n",
    "    temperature=TRAIN_CONFIG[\"temperature\"],\n",
    ")\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    list(encoder.parameters()) + list(contrastive_head.parameters()),\n",
    "    lr=TRAIN_CONFIG[\"learning_rate\"],\n",
    ")\n",
    "\n",
    "print(f\"✓ Loss function: NTXentLoss (temperature={TRAIN_CONFIG['temperature']})\")\n",
    "print(f\"✓ Optimizer: AdamW (lr={TRAIN_CONFIG['learning_rate']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319978fe",
   "metadata": {},
   "source": [
    "### 3.3 Run Contrastive Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09129279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting pretraining (5 epochs)...\n",
      "Note: Increase epochs for real training (e.g., 100-300)\n",
      "\n",
      "  Epoch 1/5: Loss = 1.9540\n",
      "  Epoch 1/5: Loss = 1.9540\n",
      "  Epoch 2/5: Loss = 1.9362\n",
      "  Epoch 2/5: Loss = 1.9362\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Encode both views\u001b[39;00m\n\u001b[0;32m     18\u001b[0m features1 \u001b[38;5;241m=\u001b[39m encoder(view1)\n\u001b[1;32m---> 19\u001b[0m features2 \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mview2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Project to embedding space\u001b[39;00m\n\u001b[0;32m     22\u001b[0m embeddings1 \u001b[38;5;241m=\u001b[39m contrastive_head(features1)\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\Y\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\my_code\\swift_pipeline\\models\\swin4d_transformer_ver7.py:894\u001b[0m, in \u001b[0;36mSwinTransformer4D.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers):\n\u001b[0;32m    893\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embeds[i](x)\n\u001b[1;32m--> 894\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[38;5;66;03m# moved this part to clf_mlp or reg_mlp\u001b[39;00m\n\u001b[0;32m    897\u001b[0m \n\u001b[0;32m    898\u001b[0m \u001b[38;5;66;03m# x = x.flatten(start_dim=2).transpose(1, 2)  # B L C\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# x = torch.flatten(x, 1)\u001b[39;00m\n\u001b[0;32m    902\u001b[0m \u001b[38;5;66;03m# x = self.head(x)\u001b[39;00m\n\u001b[0;32m    904\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\Y\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\my_code\\swift_pipeline\\models\\swin4d_transformer_ver7.py:526\u001b[0m, in \u001b[0;36mBasicLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    524\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m compute_mask([dp, hp, wp, tp], window_size, shift_size, x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m--> 526\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    527\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(b, d, h, w, t, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    528\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\Y\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\my_code\\swift_pipeline\\models\\swin4d_transformer_ver7.py:330\u001b[0m, in \u001b[0;36mSwinTransformerBlock4D.forward\u001b[1;34m(self, x, mask_matrix)\u001b[0m\n\u001b[0;32m    328\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_part1, x, mask_matrix)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 330\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_part1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_matrix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m x \u001b[38;5;241m=\u001b[39m shortcut \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path(x)\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_checkpoint:\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\my_code\\swift_pipeline\\models\\swin4d_transformer_ver7.py:304\u001b[0m, in \u001b[0;36mSwinTransformerBlock4D.forward_part1\u001b[1;34m(self, x, mask_matrix)\u001b[0m\n\u001b[0;32m    302\u001b[0m     attn_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    303\u001b[0m x_windows \u001b[38;5;241m=\u001b[39m window_partition(shifted_x, window_size)\n\u001b[1;32m--> 304\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_windows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m attn_windows \u001b[38;5;241m=\u001b[39m attn_windows\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m*\u001b[39m(window_size \u001b[38;5;241m+\u001b[39m (c,)))\n\u001b[0;32m    306\u001b[0m shifted_x \u001b[38;5;241m=\u001b[39m window_reverse(attn_windows, window_size, dims)\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\Y\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\my_code\\swift_pipeline\\models\\swin4d_transformer_ver7.py:199\u001b[0m, in \u001b[0;36mWindowAttention4D.forward\u001b[1;34m(self, x, mask)\u001b[0m\n\u001b[0;32m    195\u001b[0m     attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mview(b_ \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m nw, nw, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, n, n) \u001b[38;5;241m+\u001b[39m mask\u001b[38;5;241m.\u001b[39mto(\n\u001b[0;32m    196\u001b[0m         attn\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m    197\u001b[0m     )\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    198\u001b[0m     attn \u001b[38;5;241m=\u001b[39m attn\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, n, n)\n\u001b[1;32m--> 199\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(attn)\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\Y\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\Y\\envs\\py39\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1482\u001b[0m, in \u001b[0;36mSoftmax.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacklevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Sagi\\Desktop\\fixed_code\\Y\\envs\\py39\\lib\\site-packages\\torch\\nn\\functional.py:1843\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1842\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1843\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1844\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1845\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(f\"\\nStarting pretraining ({PRETRAIN_EPOCHS} epochs)...\")\n",
    "print(\"Note: Increase epochs for real training (e.g., 100-300)\\n\")\n",
    "\n",
    "encoder.train()\n",
    "contrastive_head.train()\n",
    "\n",
    "for epoch in range(PRETRAIN_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch_idx, (view1, view2) in enumerate(train_loader):\n",
    "        view1, view2 = view1.to(DEVICE), view2.to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Encode both views\n",
    "        features1 = encoder(view1)\n",
    "        features2 = encoder(view2)\n",
    "        \n",
    "        # Project to embedding space\n",
    "        embeddings1 = contrastive_head(features1)\n",
    "        embeddings2 = contrastive_head(features2)\n",
    "        \n",
    "        # Compute contrastive loss\n",
    "        loss = criterion(embeddings1, embeddings2)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Limit batches for demo (remove for real training)\n",
    "        if num_batches >= 10:\n",
    "            break\n",
    "    \n",
    "    avg_loss = epoch_loss / num_batches\n",
    "    print(f\"  Epoch {epoch+1}/{PRETRAIN_EPOCHS}: Loss = {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Pretraining complete!\")\n",
    "print(\"  The encoder has learned to create meaningful fMRI representations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ca44c5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9cf3482",
   "metadata": {},
   "source": [
    "### 3.4 Save Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de5a93bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Saved pretrained encoder to: ../checkpoints/pretrained_adni.pth\n",
      "  Note: Only the encoder is saved, not the contrastive head\n"
     ]
    }
   ],
   "source": [
    "pretrained_path = \"../checkpoints/pretrained_adni.pth\"\n",
    "os.makedirs(os.path.dirname(pretrained_path), exist_ok=True)\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': encoder.state_dict(),\n",
    "    'config': MODEL_CONFIG,\n",
    "}, pretrained_path)\n",
    "\n",
    "print(f\"✓ Saved pretrained encoder to: {pretrained_path}\")\n",
    "print(\"  Note: Only the encoder is saved, not the contrastive head\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4928f2d0",
   "metadata": {},
   "source": [
    "## 4. Stage 2: Supervised Fine-tuning for Degradation Prediction\n",
    "\n",
    "Now we use the pretrained encoder for the downstream task: predicting cognitive decline (degradation).\n",
    "\n",
    "Key differences from pretraining:\n",
    "- **Uses labels**: We now have degradation labels (0 or 1) from `imageID_to_labels.json`\n",
    "- **Frozen encoder**: The pretrained encoder is typically frozen (not trained)\n",
    "- **Classification head**: We train only a small classification head on top\n",
    "- **Fewer epochs needed**: Transfer learning converges faster than training from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ef7a33",
   "metadata": {},
   "source": [
    "### 4.1 Prepare Fine-tuning Datasets with Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a03145e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STAGE 2: SUPERVISED FINE-TUNING\n",
      "================================================================================\n",
      "================================================================================\n",
      "PREPARING ADNI DATA FOR SWIFT PIPELINE\n",
      "================================================================================\n",
      "Loading ADNI data...\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"STAGE 2: SUPERVISED FINE-TUNING\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Prepare datasets for finetuning with labels\n",
    "finetune_datasets = prepare_adni_datasets(\n",
    "    data_path=DATA_PATH,\n",
    "    labels_path=LABELS_PATH,\n",
    "    info_path=INFO_PATH,\n",
    "    task_names=DEGRADATION_TASKS,\n",
    "    stage=\"finetune\",\n",
    "    val_split=0.1,\n",
    "    test_split=0.2,\n",
    "    seed=42,\n",
    "    target_spatial_size=TARGET_SPATIAL_SIZE,\n",
    "    handle_nan='skip',  # Skip samples with NaN labels\n",
    ")\n",
    "\n",
    "# Create dataloaders\n",
    "finetune_loaders = create_adni_dataloaders(\n",
    "    finetune_datasets,\n",
    "    batch_size=FINETUNE_TRAIN_CONFIG[\"batch_size\"],\n",
    "    shuffle_train=True,\n",
    ")\n",
    "\n",
    "train_loader_ft = finetune_loaders[\"train_loader\"]\n",
    "val_loader_ft = finetune_loaders[\"val_loader\"]\n",
    "test_loader_ft = finetune_loaders[\"test_loader\"]\n",
    "\n",
    "print(f\"\\n✓ Dataloaders created:\")\n",
    "print(f\"  - Train batches: {len(train_loader_ft)}\")\n",
    "print(f\"  - Val batches: {len(val_loader_ft)}\")\n",
    "print(f\"  - Test batches: {len(test_loader_ft)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a04ec8f",
   "metadata": {},
   "source": [
    "### 4.2 Load Pretrained Encoder and Create Classification Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02bdbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading pretrained encoder...\")\n",
    "finetuned_encoder = SwinTransformer4D(**MODEL_CONFIG).to(DEVICE)\n",
    "checkpoint = torch.load(pretrained_path, map_location=DEVICE)\n",
    "finetuned_encoder.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\"✓ Loaded pretrained weights\")\n",
    "\n",
    "# Freeze encoder if specified\n",
    "if TASK_CONFIG['freeze_encoder']:\n",
    "    print(\"\\nFreezing encoder weights...\")\n",
    "    for param in finetuned_encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    finetuned_encoder.eval()\n",
    "    \n",
    "    trainable = sum(p.numel() for p in finetuned_encoder.parameters() if p.requires_grad)\n",
    "    total = sum(p.numel() for p in finetuned_encoder.parameters())\n",
    "    print(f\"  - Trainable encoder parameters: {trainable:,} / {total:,}\")\n",
    "else:\n",
    "    print(\"\\nEncoder weights will be fine-tuned (not frozen)\")\n",
    "\n",
    "# Create classification head\n",
    "num_tasks = len(DEGRADATION_TASKS)\n",
    "classification_head = ClassificationHead(\n",
    "    num_classes=2,  # Binary classification\n",
    "    num_features=HEAD_CONFIG['num_features']\n",
    ").to(DEVICE)\n",
    "\n",
    "head_params = sum(p.numel() for p in classification_head.parameters())\n",
    "print(f\"\\n✓ Classification head created\")\n",
    "print(f\"  - Head parameters: {head_params:,}\")\n",
    "print(f\"  - Task(s): {DEGRADATION_TASKS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f0f5b3",
   "metadata": {},
   "source": [
    "### 4.3 Setup Training for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68703ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup optimizer (only for classification head if encoder is frozen)\n",
    "if TASK_CONFIG['freeze_encoder']:\n",
    "    params_to_optimize = classification_head.parameters()\n",
    "    print(\"Optimizing: Classification head only (encoder frozen)\")\n",
    "else:\n",
    "    params_to_optimize = list(finetuned_encoder.parameters()) + list(classification_head.parameters())\n",
    "    print(\"Optimizing: Both encoder and classification head\")\n",
    "\n",
    "optimizer_ft = optim.AdamW(\n",
    "    params_to_optimize,\n",
    "    lr=FINETUNE_TRAIN_CONFIG[\"learning_rate\"],\n",
    "    weight_decay=FINETUNE_TRAIN_CONFIG[\"weight_decay\"],\n",
    ")\n",
    "\n",
    "# Loss function for binary classification\n",
    "criterion_ft = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(f\"\\n✓ Setup complete:\")\n",
    "print(f\"  - Optimizer: AdamW\")\n",
    "print(f\"  - Learning rate: {FINETUNE_TRAIN_CONFIG['learning_rate']}\")\n",
    "print(f\"  - Loss function: BCEWithLogitsLoss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11783cdd",
   "metadata": {},
   "source": [
    "### 4.4 Run Fine-tuning Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6b614",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nStarting fine-tuning ({FINETUNE_EPOCHS} epochs)...\")\n",
    "classification_head.train()\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(FINETUNE_EPOCHS):\n",
    "    # Training phase\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader_ft):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE).float().squeeze()\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer_ft.zero_grad()\n",
    "        \n",
    "        with torch.set_grad_enabled(not TASK_CONFIG['freeze_encoder']):\n",
    "            features = finetuned_encoder(inputs)\n",
    "        \n",
    "        outputs = classification_head(features).squeeze()\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion_ft(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer_ft.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        train_loss += loss.item()\n",
    "        predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        train_correct += (predictions == labels).sum().item()\n",
    "        train_total += labels.size(0)\n",
    "    \n",
    "    # Validation phase\n",
    "    classification_head.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader_ft:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE).float().squeeze()\n",
    "            \n",
    "            features = finetuned_encoder(inputs)\n",
    "            outputs = classification_head(features).squeeze()\n",
    "            \n",
    "            loss = criterion_ft(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            val_correct += (predictions == labels).sum().item()\n",
    "            val_total += labels.size(0)\n",
    "    \n",
    "    # Calculate epoch metrics\n",
    "    train_loss /= len(train_loader_ft)\n",
    "    train_acc = 100 * train_correct / train_total\n",
    "    val_loss /= len(val_loader_ft)\n",
    "    val_acc = 100 * val_correct / val_total\n",
    "    \n",
    "    print(f\"  Epoch {epoch+1}/{FINETUNE_EPOCHS}: \"\n",
    "          f\"Train Loss={train_loss:.4f}, Acc={train_acc:.2f}% | \"\n",
    "          f\"Val Loss={val_loss:.4f}, Acc={val_acc:.2f}%\")\n",
    "    \n",
    "    # Track best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_epoch = epoch + 1\n",
    "    \n",
    "    classification_head.train()\n",
    "\n",
    "print(f\"\\n✓ Fine-tuning complete!\")\n",
    "print(f\"  Best validation accuracy: {best_val_acc:.2f}% at epoch {best_epoch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fe26f2",
   "metadata": {},
   "source": [
    "## 5. Evaluation on Test Set\n",
    "\n",
    "Evaluate the fine-tuned model on the held-out test set to measure final performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f7d356",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"EVALUATION ON TEST SET\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "classification_head.eval()\n",
    "test_loss = 0.0\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "all_probabilities = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader_ft:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE).float().squeeze()\n",
    "        \n",
    "        features = finetuned_encoder(inputs)\n",
    "        outputs = classification_head(features).squeeze()\n",
    "        \n",
    "        loss = criterion_ft(outputs, labels)\n",
    "        \n",
    "        test_loss += loss.item()\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "        predictions = (probabilities > 0.5).float()\n",
    "        test_correct += (predictions == labels).sum().item()\n",
    "        test_total += labels.size(0)\n",
    "        \n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probabilities.extend(probabilities.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader_ft)\n",
    "test_acc = 100 * test_correct / test_total\n",
    "\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"  - Loss: {test_loss:.4f}\")\n",
    "print(f\"  - Accuracy: {test_acc:.2f}%\")\n",
    "print(f\"  - Correct predictions: {test_correct}/{test_total}\")\n",
    "print(f\"  - Task: {DEGRADATION_TASKS[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d605f4",
   "metadata": {},
   "source": [
    "### 5.1 Compute Additional Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e415793",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert to numpy arrays\n",
    "predictions_np = np.array(all_predictions)\n",
    "labels_np = np.array(all_labels)\n",
    "\n",
    "# Calculate confusion matrix metrics\n",
    "tp = ((predictions_np == 1) & (labels_np == 1)).sum()\n",
    "tn = ((predictions_np == 0) & (labels_np == 0)).sum()\n",
    "fp = ((predictions_np == 1) & (labels_np == 0)).sum()\n",
    "fn = ((predictions_np == 0) & (labels_np == 1)).sum()\n",
    "\n",
    "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(\"\\nDetailed Metrics:\")\n",
    "print(f\"  - Precision: {precision:.4f}\")\n",
    "print(f\"  - Recall: {recall:.4f}\")\n",
    "print(f\"  - F1-Score: {f1:.4f}\")\n",
    "print(f\"\\nConfusion Matrix:\")\n",
    "print(f\"  - True Positives: {tp}\")\n",
    "print(f\"  - True Negatives: {tn}\")\n",
    "print(f\"  - False Positives: {fp}\")\n",
    "print(f\"  - False Negatives: {fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccffabd",
   "metadata": {},
   "source": [
    "## 6. Save Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2e1d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_path = \"../checkpoints/finetuned_degradation.pth\"\n",
    "\n",
    "torch.save({\n",
    "    'encoder_state_dict': finetuned_encoder.state_dict(),\n",
    "    'head_state_dict': classification_head.state_dict(),\n",
    "    'config': {\n",
    "        'model_config': MODEL_CONFIG,\n",
    "        'task_config': TASK_CONFIG,\n",
    "        'tasks': DEGRADATION_TASKS,\n",
    "    },\n",
    "    'results': {\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'test_loss': test_loss,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "    }\n",
    "}, final_path)\n",
    "\n",
    "print(f\"✓ Saved final model to: {final_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340baf72",
   "metadata": {},
   "source": [
    "## 7. Pipeline Summary\n",
    "\n",
    "Complete overview of the SwiFT pipeline results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6c736c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"SWIFT PIPELINE COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n📊 PIPELINE SUMMARY\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n🔹 Stage 1: Contrastive Pretraining (Self-Supervised)\")\n",
    "print(f\"   - Training samples: {len(pretrain_datasets['train_dataset'])}\")\n",
    "print(f\"   - Epochs: {PRETRAIN_EPOCHS}\")\n",
    "print(f\"   - Method: Contrastive learning on temporal windows\")\n",
    "print(f\"   - Result: Pretrained encoder with learned fMRI representations\")\n",
    "\n",
    "print(\"\\n🔹 Stage 2: Supervised Fine-tuning\")\n",
    "print(f\"   - Training samples: {len(finetune_datasets['train_dataset'])}\")\n",
    "print(f\"   - Validation samples: {len(finetune_datasets['val_dataset'])}\")\n",
    "print(f\"   - Test samples: {len(finetune_datasets['test_dataset'])}\")\n",
    "print(f\"   - Epochs: {FINETUNE_EPOCHS}\")\n",
    "print(f\"   - Task: {DEGRADATION_TASKS[0]}\")\n",
    "\n",
    "print(\"\\n🎯 FINAL RESULTS\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "print(f\"   Test Accuracy:            {test_acc:.2f}%\")\n",
    "print(f\"   Test Precision:           {precision:.4f}\")\n",
    "print(f\"   Test Recall:              {recall:.4f}\")\n",
    "print(f\"   Test F1-Score:            {f1:.4f}\")\n",
    "\n",
    "print(\"\\n💾 SAVED MODELS\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   Pretrained encoder: {pretrained_path}\")\n",
    "print(f\"   Fine-tuned model:   {final_path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✓ SwiFT pipeline successfully completed!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcb13cc",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "### For Better Performance:\n",
    "1. **Increase training epochs**:\n",
    "   - Pretraining: 100-300 epochs\n",
    "   - Fine-tuning: 50-100 epochs\n",
    "\n",
    "2. **Hyperparameter tuning**:\n",
    "   - Learning rates\n",
    "   - Temperature for contrastive loss\n",
    "   - Batch size\n",
    "\n",
    "3. **Data augmentation**:\n",
    "   - Add spatial and temporal augmentations\n",
    "   - Helps with generalization\n",
    "\n",
    "4. **Multi-task learning**:\n",
    "   - Train on multiple degradation horizons simultaneously\n",
    "   - Can improve overall performance\n",
    "\n",
    "### Using the Trained Model:\n",
    "\n",
    "```python\n",
    "# Load the fine-tuned model\n",
    "checkpoint = torch.load('../checkpoints/finetuned_degradation.pth')\n",
    "\n",
    "# Initialize models\n",
    "encoder = SwinTransformer4D(**checkpoint['config']['model_config'])\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "\n",
    "head = ClassificationHead(num_classes=2, num_features=288)\n",
    "head.load_state_dict(checkpoint['head_state_dict'])\n",
    "\n",
    "# Make predictions\n",
    "encoder.eval()\n",
    "head.eval()\n",
    "with torch.no_grad():\n",
    "    features = encoder(new_data)\n",
    "    predictions = torch.sigmoid(head(features))\n",
    "```\n",
    "\n",
    "### Documentation:\n",
    "- Full documentation: `data/README_ADNI.md`\n",
    "- Dataset classes: `data/dataset_adni.py`\n",
    "- Data preparation: `data/prepare_adni_data.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
